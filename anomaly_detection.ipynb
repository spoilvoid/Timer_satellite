{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6c3d9be",
   "metadata": {},
   "source": [
    "### 依赖包导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "306f5693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "from exp.exp_forecast import Exp_Forecast\n",
    "from exp.exp_anomaly_detection import Exp_Anomaly_Detection\n",
    "from exp.exp_imputation import Exp_Imputation\n",
    "from utils.tools import HiddenPrints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff20dc79",
   "metadata": {},
   "source": [
    "### 变量定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8079666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = argparse.Namespace(\n",
    "    # ===== basic config =====\n",
    "    task_name=\"anomaly_detection\",\n",
    "    model_id=\"Timer_multivariate_anomaly_detection\",\n",
    "    model=\"Timer_multivariate\",\n",
    "    seed=42,\n",
    "\n",
    "    # ===== data loader =====\n",
    "    data=\"multivariate_anomaly\",\n",
    "    root_path=\"./dataset/xw/elec\",\n",
    "    data_path=\"ETTh1.csv\",\n",
    "    features=\"M\",\n",
    "    target=\"OT\",\n",
    "    freq=\"h\",\n",
    "    checkpoints=\"./checkpoints/\",\n",
    "    inverse=False,\n",
    "\n",
    "    # ===== model define =====\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    "    e_layers=4,\n",
    "    d_layers=1,\n",
    "    d_ff=512,\n",
    "    factor=1,\n",
    "    distil=True,\n",
    "    dropout=0.1,\n",
    "    embed=\"timeF\",\n",
    "    activation=\"gelu\",\n",
    "    output_attention=False,\n",
    "    use_norm=True,\n",
    "    max_len=10000,\n",
    "    mask_flag=True,\n",
    "    binary_bias=False,\n",
    "    covariate=True,\n",
    "    n_pred_vars=15,\n",
    "    freeze_layer=False,\n",
    "\n",
    "    # ===== optimization =====\n",
    "    num_workers=10,\n",
    "    itr=1,\n",
    "    train_epochs=10,\n",
    "    batch_size=64,\n",
    "    patience=3,\n",
    "    learning_rate=1e-4,\n",
    "    des=\"test\",\n",
    "    loss=\"MSE\",\n",
    "    lradj=\"type1\",\n",
    "    use_amp=False,\n",
    "\n",
    "    # ===== GPU =====\n",
    "    use_gpu=True,\n",
    "    gpu=0,\n",
    "    use_multi_gpu=False,\n",
    "    devices=\"0,1,2,3\",\n",
    "\n",
    "    # ===== misc =====\n",
    "    stride=1,\n",
    "    ckpt_path=\"checkpoints/Timer_anomaly_detection_1.0.ckpt\",\n",
    "    finetune_epochs=10,\n",
    "    finetune_rate=0.1,\n",
    "    local_rank=0,\n",
    "\n",
    "    patch_len=96,\n",
    "    subset_rand_ratio=1.0,\n",
    "    data_type=\"custom\",\n",
    "\n",
    "    decay_fac=0.75,\n",
    "\n",
    "    # ===== cosine decay =====\n",
    "    cos_warm_up_steps=100,\n",
    "    cos_max_decay_steps=60000,\n",
    "    cos_max_decay_epoch=10,\n",
    "    cos_max=1e-4,\n",
    "    cos_min=2e-6,\n",
    "\n",
    "    # ===== weight decay =====\n",
    "    use_weight_decay=0,\n",
    "    weight_decay=0.01,\n",
    "\n",
    "    # ===== autoregressive configs =====\n",
    "    use_ims=True,\n",
    "    output_len=96,\n",
    "    output_len_list=None,\n",
    "\n",
    "    # ===== train_test =====\n",
    "    train_test=0,\n",
    "    valid_ratio=0.2,\n",
    "    is_finetuning=1,\n",
    "    test_dir=\"test_results\",\n",
    "    test_version=\"test\",  # 可选 \"test\", \"predict\", \"prune\", \"visualize\"\n",
    "    prune_ratio=0.2,\n",
    "    remove_mask=False,\n",
    "\n",
    "    # ===== forecasting task =====\n",
    "    seq_len=768,\n",
    "    label_len=48,\n",
    "    pred_len=96,\n",
    "    input_len=96,\n",
    "\n",
    "    # ===== imputation task =====\n",
    "    mask_rate=0.25,\n",
    "\n",
    "    # ===== anomaly detection task =====\n",
    "    loss_threshold=10.0,\n",
    "\n",
    "    # ===== opacus options =====\n",
    "    use_opacus=False,\n",
    "    noise_multiplier=1.1,\n",
    "    max_grad_norm=1.0,\n",
    "\n",
    "    # ===== training info visualize configs =====\n",
    "    record_info=False,\n",
    ")\n",
    "\n",
    "fix_seed = args.seed\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)\n",
    "args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "if args.use_multi_gpu:\n",
    "    ip = os.environ.get(\"MASTER_ADDR\", \"127.0.0.1\")\n",
    "    port = os.environ.get(\"MASTER_PORT\", \"64209\")\n",
    "    hosts = int(os.environ.get(\"WORLD_SIZE\", \"8\"))  # number of nodes\n",
    "    rank = int(os.environ.get(\"RANK\", \"0\"))  # node id\n",
    "    local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n",
    "    gpus = torch.cuda.device_count()  # gpus per node\n",
    "    args.local_rank = local_rank\n",
    "    print(\n",
    "        'ip: {}, port: {}, hosts: {}, rank: {}, local_rank: {}, gpus: {}'.format(ip, port, hosts, rank, local_rank,\n",
    "                                                                                    gpus))\n",
    "    dist.init_process_group(backend=\"nccl\", init_method=f\"tcp://{ip}:{port}\", world_size=hosts, rank=rank)\n",
    "    print('init_process_group finished')\n",
    "    torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab7edbf",
   "metadata": {},
   "source": [
    "### 训练数据读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84689300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoints folder are $root/checkpoints/Timer_multivariate_anomaly_detection_multivariate_anomaly_d256_n8_l4_itr0_25-09-07_16-59-00.\n",
      "test_results folder are $root/test_results/Timer_multivariate_anomaly_detection_multivariate_anomaly_d256_n8_l4_itr0_25-09-07_16-59-00.\n",
      "Use GPU: cuda:0\n",
      "Number of samples in trainset: 272\n",
      "Number of batches in trainloader: 5\n",
      "First sample in trainset: [[  0   0   0 ...   0   0   0]\n",
      " [  1   1   1 ...   1   1   1]\n",
      " [  2   2   2 ...   2   2   2]\n",
      " ...\n",
      " [765 765 765 ... 765 765 765]\n",
      " [766 766 766 ... 766 766 766]\n",
      " [767 767 767 ... 767 767 767]]\n"
     ]
    }
   ],
   "source": [
    "with HiddenPrints(int(os.environ.get(\"LOCAL_RANK\", \"0\"))):\n",
    "    # setting record of experiments\n",
    "    setting = f\"{args.model}_{args.task_name}_{args.data}_d{args.d_model}_n{args.n_heads}_l{args.e_layers}_itr0_\"\n",
    "    setting += datetime.now().strftime(\"%y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "    print(f\"checkpoints folder are $root/checkpoints/{setting}.\")\n",
    "    print(f\"test_results folder are $root/test_results/{setting}.\")\n",
    "    \n",
    "    exp = Exp_Anomaly_Detection(args)  # set experiments\n",
    "    trainset, trainloader = exp._get_data(flag=\"train\")\n",
    "    print(f\"Number of samples in trainset: {len(trainset)}\")\n",
    "    print(f\"Number of batches in trainloader: {len(trainloader)}\")\n",
    "    first_sample = trainset[0]\n",
    "    print(\"First sample in trainset:\", first_sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55077258",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02906c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>start training : Timer_multivariate_anomaly_detection_multivariate_anomaly_d256_n8_l4_itr0_25-09-07_16-47-43>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train steps per epoch: 5\n",
      "Epoch: 1 cost time: 5.019757986068726\n",
      "Epoch: 1, Steps: 5 | Train Loss: 43764.3093750 Vali Loss: 28524.4746094\n",
      "Validation loss decreased (inf --> 28524.474609).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "Epoch: 2 cost time: 0.988797664642334\n",
      "Epoch: 2, Steps: 5 | Train Loss: 25775.9738281 Vali Loss: 16855.4785156\n",
      "Validation loss decreased (28524.474609 --> 16855.478516).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "Epoch: 3 cost time: 0.9845714569091797\n",
      "Epoch: 3, Steps: 5 | Train Loss: 17668.7078125 Vali Loss: 13245.5869141\n",
      "Validation loss decreased (16855.478516 --> 13245.586914).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "Epoch: 4 cost time: 0.9728877544403076\n",
      "Epoch: 4, Steps: 5 | Train Loss: 14839.4644531 Vali Loss: 11812.5869141\n",
      "Validation loss decreased (13245.586914 --> 11812.586914).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "Epoch: 5 cost time: 1.011564016342163\n",
      "Epoch: 5, Steps: 5 | Train Loss: 13632.1933594 Vali Loss: 11166.9140625\n",
      "Validation loss decreased (11812.586914 --> 11166.914062).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      "Epoch: 6 cost time: 1.006880521774292\n",
      "Epoch: 6, Steps: 5 | Train Loss: 13100.7566406 Vali Loss: 10861.1835938\n",
      "Validation loss decreased (11166.914062 --> 10861.183594).  Saving model ...\n",
      "Updating learning rate to 3.125e-06\n",
      "Epoch: 7 cost time: 0.9873499870300293\n",
      "Epoch: 7, Steps: 5 | Train Loss: 12826.4375000 Vali Loss: 10712.6777344\n",
      "Validation loss decreased (10861.183594 --> 10712.677734).  Saving model ...\n",
      "Updating learning rate to 1.5625e-06\n",
      "Epoch: 8 cost time: 1.0030055046081543\n",
      "Epoch: 8, Steps: 5 | Train Loss: 12711.2285156 Vali Loss: 10639.5820312\n",
      "Validation loss decreased (10712.677734 --> 10639.582031).  Saving model ...\n",
      "Updating learning rate to 7.8125e-07\n",
      "Epoch: 9 cost time: 0.9912550449371338\n",
      "Epoch: 9, Steps: 5 | Train Loss: 12645.5320313 Vali Loss: 10603.1220703\n",
      "Validation loss decreased (10639.582031 --> 10603.122070).  Saving model ...\n",
      "Updating learning rate to 3.90625e-07\n",
      "Epoch: 10 cost time: 0.9797136783599854\n",
      "Epoch: 10, Steps: 5 | Train Loss: 12615.8685547 Vali Loss: 10584.8710938\n",
      "Validation loss decreased (10603.122070 --> 10584.871094).  Saving model ...\n",
      "Updating learning rate to 1.953125e-07\n"
     ]
    }
   ],
   "source": [
    "with HiddenPrints(int(os.environ.get(\"LOCAL_RANK\", \"0\"))):\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.finetune(setting)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e091b11a",
   "metadata": {},
   "source": [
    "### 测试数据生成和异常数据保存（可选，如需要请取消注释代码，保存位置在checkpoints保存位置的文件夹下）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4117a42c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>testing : Timer_multivariate_anomaly_detection_multivariate_anomaly_d256_n8_l4_itr0_25-09-07_16-47-43<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "-------testing : test.csv-------\n",
      "(tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  1.,   1.,   1.,  ...,   1.,   1.,   1.],\n",
      "        [  2.,   2.,   2.,  ...,   2.,   2.,   2.],\n",
      "        ...,\n",
      "        [765., 765., 765.,  ..., 765., 765., 765.],\n",
      "        [766., 766., 766.,  ..., 766., 766., 766.],\n",
      "        [767., 767., 767.,  ..., 767., 767., 767.]]),)\n"
     ]
    }
   ],
   "source": [
    "with HiddenPrints(int(os.environ.get(\"LOCAL_RANK\", \"0\"))):\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    filename_list = os.listdir(os.path.join(args.root_path, \"test\"))\n",
    "    for filename in filename_list:\n",
    "        args.data_path = filename\n",
    "        print(f\"-------testing : {filename}-------\")\n",
    "        anomaly_dataset, _ = exp._gen_manual_anomaly_patch(k=5)\n",
    "        tensor = anomaly_dataset.tensors[0] if hasattr(anomaly_dataset, \"tensors\") else anomaly_dataset.tensor\n",
    "        # 保存为 PyTorch 二进制文件\n",
    "        # torch.save(tensor, os.path.join(args.checkpoints, setting, f\"{filename}.pt\"))\n",
    "        torch.cuda.empty_cache()\n",
    "print(anomaly_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e910ed8e",
   "metadata": {},
   "source": [
    "### 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02497a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>testing : Timer_multivariate_anomaly_detection_multivariate_anomaly_d256_n8_l4_itr0_25-09-07_16-47-43<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "-------testing : test.csv-------\n",
      "Model parameters:  2158176\n"
     ]
    }
   ],
   "source": [
    "with HiddenPrints(int(os.environ.get(\"LOCAL_RANK\", \"0\"))):\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    filename_list = os.listdir(os.path.join(args.root_path, \"test\"))\n",
    "    for filename in filename_list:\n",
    "        args.data_path = filename\n",
    "        print(f\"-------testing : {filename}-------\")\n",
    "        exp.test(setting)\n",
    "        torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_timer_xl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

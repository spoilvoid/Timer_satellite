import random
import numpy as np
import streamlit as st
import torch
import os
import argparse
from datetime import datetime
import multiprocessing as mp
import atexit
import os, subprocess, socket
import streamlit.components.v1 as components

from exp.exp_forecast import Exp_Forecast
from utils.tools import HiddenPrints


def get_available_devices():
    """æ£€æµ‹å¹¶è¿”å›å¯ç”¨çš„PyTorchè®¾å¤‡åˆ—è¡¨"""
    devices = ["cpu"]
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            devices.append(i)
    return devices

@st.cache_resource
def launch_tensorboard(logdir):
    """
    å¯åŠ¨ä¸€ä¸ªç‹¬ç«‹çš„ TensorBoard è¿›ç¨‹ï¼Œå¹¶ç¡®ä¿å®ƒåœ¨ Streamlit é€€å‡ºæ—¶è¢«æ¸…ç†ã€‚
    ä½¿ç”¨ @st.cache_resource ç¡®ä¿æ­¤å‡½æ•°åœ¨æ•´ä¸ªåº”ç”¨ç”Ÿå‘½å‘¨æœŸä¸­åªè¿è¡Œä¸€æ¬¡ã€‚
    """
    with socket.socket() as s:
        s.bind(('', 0))
        port = s.getsockname()[1]
    
    print(f"æ­£åœ¨ç«¯å£ {port} ä¸Šå¯åŠ¨ TensorBoard...")
    command = [
        "tensorboard",
        "--logdir", logdir,
        "--host", "0.0.0.0",
        "--port", str(port),
    ]
    p = subprocess.Popen(command)
    
    atexit.register(p.terminate)
    
    return port

def training_process_wrapper(params: dict):
    """
    å°è£…äº† train_TFdecoder çš„å‡½æ•°ï¼Œä½¿å…¶å¯ä»¥åœ¨ç‹¬ç«‹çš„è¿›ç¨‹ä¸­è¿è¡Œã€‚
    æ¥æ”¶ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¶…å‚æ•°çš„å­—å…¸ã€‚
    """
    try:
        args = argparse.Namespace(
            # ===== basic config =====
            task_name="forecast",
            model_id="Timer_multivariate_forecast",
            model="Timer_multivariate",
            seed=42,

            # ===== data loader =====
            data="multivariate",
            root_path="./dataset/xw/elec",
            data_path="ETTh1.csv",
            features="M",
            target="OT",
            freq="h",
            checkpoints="./checkpoints/",
            inverse=False,

            # ===== model define =====
            d_model=1024,
            n_heads=8,
            e_layers=8,
            d_layers=1,
            d_ff=2048,
            factor=3,
            distil=True,
            dropout=0.1,
            embed="timeF",
            activation="gelu",
            output_attention=False,
            use_norm=True,
            max_len=10000,
            mask_flag=True,
            binary_bias=False,
            covariate=True,
            n_pred_vars=15,
            freeze_layer=False,

            # ===== optimization =====
            num_workers=4,
            itr=1,
            train_epochs=10,
            batch_size=32,
            patience=3,
            learning_rate=3e-5,
            des="Exp",
            loss="MSE",
            lradj="type1",
            use_amp=False,

            # ===== GPU =====
            use_gpu=True,
            gpu=0,
            use_multi_gpu=False,
            devices="0,1,2,3",

            # ===== misc =====
            stride=1,
            ckpt_path="checkpoints/original/Timer_forecast_1.0.ckpt",
            finetune_epochs=10,
            finetune_rate=0.1,
            local_rank=0,

            patch_len=96,
            subset_rand_ratio=1.0,
            data_type="custom",

            decay_fac=0.75,

            # ===== cosine decay =====
            cos_warm_up_steps=100,
            cos_max_decay_steps=60000,
            cos_max_decay_epoch=10,
            cos_max=1e-4,
            cos_min=2e-6,

            # ===== weight decay =====
            use_weight_decay=0,
            weight_decay=0.01,

            # ===== autoregressive configs =====
            use_ims=True,
            output_len=96,
            output_len_list=None,

            # ===== train_test =====
            train_test=0,
            valid_ratio=0.2,
            is_finetuning=1,
            test_dir="test_results",
            test_version="test",  # å¯é€‰ "test", "predict", "prune", "visualize"
            prune_ratio=0.2,
            remove_mask=False,

            # ===== forecasting task =====
            seq_len=672,
            label_len=576,
            pred_len=96,
            input_len=96,

            # ===== imputation task =====
            mask_rate=0.25,

            # ===== anomaly detection task =====
            loss_threshold=10.0,

            # ===== opacus options =====
            use_opacus=False,
            noise_multiplier=1.1,
            max_grad_norm=1.0,

            # ===== training info visualize configs =====
            record_info=False,

            # ===== version info =====
            model_version=1,
        )

        for key, value in params.items():
            if key in args.__dict__:
                args.__dict__[key] = value
            else:
                print(f"è­¦å‘Š: æœªçŸ¥å‚æ•° '{key}'ï¼Œå°†è¢«å¿½ç•¥ã€‚")

        fix_seed = args.seed
        random.seed(fix_seed)
        torch.manual_seed(fix_seed)
        np.random.seed(fix_seed)
        args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False
        if args.use_multi_gpu:
            ip = os.environ.get("MASTER_ADDR", "127.0.0.1")
            port = os.environ.get("MASTER_PORT", "64209")
            hosts = int(os.environ.get("WORLD_SIZE", "8"))  # number of nodes
            rank = int(os.environ.get("RANK", "0"))  # node id
            local_rank = int(os.environ.get("LOCAL_RANK", "0"))
            gpus = torch.cuda.device_count()  # gpus per node
            args.local_rank = local_rank
            print(
                'ip: {}, port: {}, hosts: {}, rank: {}, local_rank: {}, gpus: {}'.format(ip, port, hosts, rank, local_rank,
                                                                                            gpus))
            torch.dist.init_process_group(backend="nccl", init_method=f"tcp://{ip}:{port}", world_size=hosts, rank=rank)
            print('init_process_group finished')
            torch.cuda.set_device(local_rank)
        with HiddenPrints(int(os.environ.get("LOCAL_RANK", "0"))):
            # setting record of experiments
            setting = f"{args.model}_{args.task_name}_{args.data}_d{args.d_model}_n{args.n_heads}_l{args.e_layers}_v{args.model_version}"
            exp = Exp_Forecast(args) 
            print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))
            exp.finetune(setting)
        # train_data_path = os.path.join(params['train_dataset_dir'], params['train_data_filename'])
        # train_dataset_dict = torch.load(train_data_path)

        # combined_dataset = train_dataset_dict["combined_dataset"]
        # temp_cols = train_dataset_dict['Y_ID_num']
        # total_features = train_dataset_dict['X_ID_num'] + train_dataset_dict['Y_ID_num']
        
        # print("å­è¿›ç¨‹ï¼šæ•°æ®åŠ è½½æˆåŠŸã€‚")

        # train_TFdecoder(
        #     training_style=params['training_style'],
        #     combined_dataset=combined_dataset,
        #     batch_size=params['batch_size'],
        #     total_features=total_features,
        #     lr=params['lr'],
        #     temp_cols=temp_cols,
        #     d_model=params['d_model'],
        #     nhead=params['nhead'],
        #     num_layers=params['num_layers'],
        #     num_epochs=params['num_epochs'], 
        #     device=torch.device(params['device']),
        #     save_path=params['save_path'],
        #     model_name=params['model_name'],
        #     version=params['version'],
        #     log_every_n_steps=params['log_every_n_steps'],
        #     checkpoint_path=params.get('checkpoint_path', None) 
        # )
        print("å­è¿›ç¨‹ï¼šè®­ç»ƒå®Œæˆã€‚")
        
    except Exception as e:
        print(f"å­è¿›ç¨‹å‘ç”Ÿé”™è¯¯: {e}")
    

# -----------------------------------------------------------------------------
# Streamlit UI ç•Œé¢
# -----------------------------------------------------------------------------

st.set_page_config(layout="wide", page_title="é€šç”¨æ¨¡å‹è®­ç»ƒæ§åˆ¶ä¸ç›‘æ§é¢æ¿")

st.title("ğŸ›°ï¸ğŸŒ¡ï¸  é€šç”¨æ¨¡å‹è®­ç»ƒæ§åˆ¶ä¸ç›‘æ§é¢æ¿")

# --- Session State åˆå§‹åŒ– ---
if 'training_process' not in st.session_state:
    st.session_state.training_process = None
if 'is_training' not in st.session_state:
    st.session_state.is_training = False
if 'current_run_name' not in st.session_state:
    st.session_state.current_run_name = ""
if 'params' not in st.session_state:
    st.session_state.params = {}

ckpt_base_dir = "./checkpoints"
ckpt_default_folder_name = "original"
dataset_base_dir = "./dataset/xw/"
dataset_default_folder_name = "board"
# --- æ–‡ä»¶ç®¡ç†ä¸é€‰æ‹© ---
st.sidebar.header("ğŸ“‚ æ•°æ®é›†ç›®å½•åˆ›å»º")
text = st.sidebar.text_input("è¾“å…¥æ•°æ®é›†åç§°", key="dataset_name", value="board", help="è¯·è¾“å…¥æ‚¨æƒ³åˆ›å»ºçš„æ•°æ®é›†åç§°ï¼Œä¾‹å¦‚ 'my_dataset'ã€‚")

if st.sidebar.button("æäº¤ç”Ÿæˆ"):
    # æŠŠ st.session_state["user_text"] ä½œä¸ºåç«¯è¯»å…¥å€¼
    dataset_folder = st.session_state["dataset_name"]
    try:
        train_files_dir = os.path.join(dataset_base_dir, dataset_folder, "trainval")
        os.makedirs(os.path.join(dataset_base_dir, dataset_folder, "trainval"), exist_ok=True)
    except Exception as e:
        st.sidebar.error(f"æ•°æ®é›†ç›®å½•åˆ›å»ºå¤±è´¥: {e}")


# --- æ–‡ä»¶ç®¡ç†ä¸é€‰æ‹© ---
st.sidebar.header("ğŸ“‚ æ–‡ä»¶ç®¡ç†")
if 'dataset_name' in st.session_state and st.session_state['dataset_name']:
    dataset_folder = st.session_state['dataset_name']
    train_files_dir = os.path.join(dataset_base_dir, dataset_folder, "trainval")
else:
    train_files_dir = os.path.join(dataset_base_dir, dataset_default_folder_name, "trainval")
os.makedirs(train_files_dir, exist_ok=True)
uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ æ–°çš„è®­ç»ƒæ–‡ä»¶ (csv)", type=['csv'], key="general_train_data_uploader")

default_train_filename = 'train.csv'

if uploaded_file is not None:
    save_path = os.path.join(train_files_dir, uploaded_file.name)
    try:
        with open(save_path, "wb") as f:
            f.write(uploaded_file.getvalue())
        st.sidebar.success(f"æ–‡ä»¶ '{uploaded_file.name}' ä¸Šä¼ æˆåŠŸï¼")
        default_train_filename = uploaded_file.name
    except Exception as e:
        st.sidebar.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")

try:
    dataset_available_files = sorted([f for f in os.listdir(dataset_base_dir) if os.path.isdir(os.path.join(dataset_base_dir, f))])
    if not dataset_available_files:
        st.sidebar.warning("è®­ç»ƒæ•°æ®ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶å¤¹ã€‚")
        st.stop()

    dataset_default_index = dataset_available_files.index(dataset_default_folder_name) if dataset_default_folder_name in dataset_available_files else 0
    
    dataset_folder = st.sidebar.selectbox(
        "é€‰æ‹©è®­ç»ƒæ–‡ä»¶å¤¹",
        dataset_available_files,
        index=dataset_default_index,
        help="é€‰æ‹©ç”¨äºæœ¬æ¬¡æ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†æ–‡ä»¶å¤¹(è®­ç»ƒåŒ…å«æ–‡ä»¶å¤¹å†…trainvalå­ç›®å½•ä¸‹çš„æ‰€æœ‰csvæ–‡ä»¶ï¼Œéœ€è¦ä¿è¯åˆ—ç›¸åŒå¦åˆ™ä¼šæŠ¥é”™)ã€‚"
    )
except FileNotFoundError:
    st.sidebar.error(f"è®­ç»ƒæ•°æ®ç›®å½•æœªæ‰¾åˆ°: {train_files_dir}")
    st.stop()

# # --- checkpointsç®¡ç†ä¸å¯¼å…¥ ---
# st.sidebar.header("ğŸ“‚ Checkpointså¯¼å…¥")
# if 'dataset_name' in st.session_state and st.session_state['dataset_name']:
#     processed = st.session_state['dataset_name'].upper()
#     train_files_dir = os.path.join("./dataset/xw/", processed, "trainval")
# else:
#     train_files_dir = './dataset/xw/board/trainval'
# os.makedirs(train_files_dir, exist_ok=True)
# uploaded_file = st.sidebar.file_uploader("ä¸Šä¼ æ–°çš„è®­ç»ƒæ–‡ä»¶ (csv)", type=['csv'], key="general_train_data_uploader")

# default_train_filename = 'train.csv'

# if uploaded_file is not None:
#     save_path = os.path.join(train_files_dir, uploaded_file.name)
#     try:
#         with open(save_path, "wb") as f:
#             f.write(uploaded_file.getvalue())
#         st.sidebar.success(f"æ–‡ä»¶ '{uploaded_file.name}' ä¸Šä¼ æˆåŠŸï¼")
#         default_train_filename = uploaded_file.name
#     except Exception as e:
#         st.sidebar.error(f"æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")

# try:
#     available_files = sorted([f for f in os.listdir(train_files_dir) if f.endswith('.csv') and os.path.isfile(os.path.join(train_files_dir, f))])
#     if not available_files:
#         st.sidebar.warning("è®­ç»ƒæ•°æ®ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½• .csv æ–‡ä»¶ã€‚")
#         st.stop()

#     default_index = available_files.index(default_train_filename) if default_train_filename in available_files else 0
    
#     selected_train_file = st.sidebar.selectbox(
#         "é€‰æ‹©è®­ç»ƒæ–‡ä»¶",
#         available_files,
#         index=default_index,
#         help="é€‰æ‹©ç”¨äºæœ¬æ¬¡æ¨¡å‹è®­ç»ƒçš„æ•°æ®é›†æ–‡ä»¶(æ–‡ä»¶å¤¹å†…æœ‰åŒåæ–‡ä»¶åˆ™æ›¿æ¢ï¼Œå¦åˆ™ä¸ºæ·»åŠ )ã€‚"
#     )
# except FileNotFoundError:
#     st.sidebar.error(f"è®­ç»ƒæ•°æ®ç›®å½•æœªæ‰¾åˆ°: {train_files_dir}")
#     st.stop()


# --- ä¾§è¾¹æ ï¼šè¶…å‚æ•°é…ç½® ---
st.sidebar.title("ğŸ› ï¸ å‚æ•°é…ç½®")

available_devices = get_available_devices()

st.sidebar.header("ç»§ç»­è®­ç»ƒè®¾ç½®")
st.sidebar.toggle(
        "ç»§ç»­è®­ç»ƒ (Continue Training)", 
        key = 'continue_training',
        help="æ¿€æ´»æ­¤é€‰é¡¹ä»¥ä»ä¸€ä¸ªå·²æœ‰çš„æ¨¡å‹æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒã€‚"
    )
try:
    ckpt_available_files = sorted([f for f in os.listdir(ckpt_base_dir) if os.path.isdir(os.path.join(ckpt_base_dir, f))])
    if not ckpt_available_files:
        st.sidebar.warning("æ¨¡å‹æƒé‡ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶å¤¹ã€‚")
        st.stop()

    ckpt_default_index = ckpt_available_files.index(ckpt_default_folder_name) if ckpt_default_folder_name in ckpt_available_files else 0
    
    ckpt_folder = st.sidebar.selectbox(
        "é€‰æ‹©æ¨¡å‹æ–‡ä»¶å¤¹",
        ckpt_available_files,
        index=ckpt_default_index,
        help="é€‰æ‹©ç”¨äºæœ¬æ¬¡æ¨¡å‹è®­ç»ƒçš„æ¨¡å‹æƒé‡æ–‡ä»¶å¤¹ã€‚",
        disabled=not st.session_state.get('continue_training', False)
    )
    if ckpt_folder == ckpt_default_folder_name:
        checkpoint_path_input = os.path.join(ckpt_base_dir, ckpt_folder, "Timer_forecast_1.0.ckpt")
    else:
        checkpoint_path_input = os.path.join(ckpt_base_dir, ckpt_folder, "checkpoint.pth")
except FileNotFoundError:
    st.sidebar.error(f"è®­ç»ƒæ•°æ®ç›®å½•æœªæ‰¾åˆ°: {os.path.join(ckpt_base_dir, ckpt_folder)}")
    st.stop()

with st.sidebar.form(key='general_params_form'):
    st.header("è¿è¡Œè®¾ç½®")
    model_name = st.text_input("æ¨¡å‹åç§° (Model Name)", value="Timer_multivariate")
    version = st.number_input("ç‰ˆæœ¬å· (Version)", min_value=1, value=1)
    device = st.selectbox("è®­ç»ƒè®¾å¤‡ (Device)", options=available_devices, index=1 if len(available_devices) > 1 else 0)

    st.header("è®­ç»ƒè¶…å‚æ•°")
    lr = st.number_input("å­¦ä¹ ç‡ (Learning Rate)", min_value=1e-6, max_value=1e-2, value=3e-5, format="%.1e")
    num_epochs = st.number_input("è®­ç»ƒè½®æ•° (Epochs)", min_value=1, max_value=5000, value=10)
    batch_size = st.select_slider("æ‰¹å¤„ç†å¤§å° (Batch Size)", options=[1, 2, 4, 8, 16, 32, 64, 128, 256, 512], value=8)

    st.header("æ¨¡å‹æ¶æ„ (Transformer-Decoder)")
    d_model = st.select_slider("æ¨¡å‹ç»´åº¦ (d_model)", options=[128, 256, 512, 1024], value=1024)
    nhead = st.select_slider("æ³¨æ„åŠ›å¤´æ•° (nhead)", options=[4, 8, 16], value=8)
    num_layers = st.number_input("å±‚æ•° (Num Layers)", min_value=1, max_value=12, value=8)
    
    submit_button = st.form_submit_button(label='é”å®šå‚æ•°å¹¶å‡†å¤‡è®­ç»ƒ')


# --- ä¸»é¢æ¿ï¼šæ§åˆ¶ä¸ç›‘æ§ ---
col1, col2 = st.columns(2)

with col1:
    st.subheader("â–¶ï¸ è®­ç»ƒæ§åˆ¶")
    control_placeholder = st.empty()
    control_placeholder.info("å¦‚æœä¸éœ€è¦åå°è®­ç»ƒæ¨¡å‹ï¼Œåˆ·æ–°æˆ–å…³é—­æµè§ˆå™¨å‰è¯·åœæ­¢è®­ç»ƒï¼Œé˜²æ­¢å‡ºç°æœªå—æ§è¿›ç¨‹ã€‚")
    start_button = st.button("å¯åŠ¨è®­ç»ƒ", type="primary", disabled=st.session_state.is_training)
    stop_button = st.button("åœæ­¢è®­ç»ƒ", disabled=not st.session_state.is_training)

with col2:
    st.subheader("ğŸ“ˆ çŠ¶æ€ä¿¡æ¯")
    status_placeholder = st.empty()
    if st.session_state.is_training:
        device_info = st.session_state.params.get('device', 'N/A')
        model_info = st.session_state.params.get('model_type', 'N/A')
        status_placeholder.success(
            f"âœ… **{model_info}** æ¨¡å‹è®­ç»ƒä¸­...\n\n"
            f"è®¾å¤‡: `{device_info}`\n\n"
            f"æ—¥å¿—ç›®å½•: `runs/{st.session_state.current_run_name}`\n\n"
            f"è®­ç»ƒæ•°æ®: `{st.session_state.params.get('train_data_filename', 'N/A')}`"
        )
    else:
        status_placeholder.info("â¹ï¸ è®­ç»ƒå·²åœæ­¢æˆ–æœªå¼€å§‹ã€‚")

# --- æŒ‰é’®é€»è¾‘å¤„ç† ---
if start_button:
    save_dir = "./checkpoints"
    os.makedirs(save_dir, exist_ok=True)
    
    st.session_state.current_run_name = f"{model_name}_v{version}"

    params = {
        "model_id": model_name,
        'batch_size': batch_size,
        'learning_rate': lr,
        'd_model': d_model,
        'n_heads': nhead,
        'e_layers': num_layers,
        'finetune_epochs': num_epochs,
        'root_path': os.path.join(dataset_base_dir, dataset_folder) if dataset_folder in os.listdir(dataset_base_dir) else os.path.join(dataset_base_dir, 'board'),
        'ckpt_path': checkpoint_path_input,
        'gpu': device,
        'model_version': version,
    }
    st.session_state.params = params

    if mp.get_start_method(allow_none=True) != 'spawn':
        mp.set_start_method('spawn', force=True)
    
    p = mp.Process(target=training_process_wrapper, args=(params,))
    p.start()
    
    st.session_state.training_process = p
    st.session_state.is_training = True
    
    st.session_state.training_process.join()

    st.session_state.training_process = None
    st.session_state.is_training = False

    st.rerun()

# åœæ­¢è®­ç»ƒ
if stop_button:
    if st.session_state.training_process is not None:
        st.session_state.training_process.terminate()
        st.session_state.training_process.join()
        st.warning(f"è®­ç»ƒè¿›ç¨‹ {st.session_state.training_process.pid} å·²è¢«ç»ˆæ­¢ã€‚")

    st.session_state.is_training = False
    st.session_state.training_process = None
    
    st.rerun()

# # --- TensorBoard é›†æˆ ---
# st.divider()
# st.subheader("ğŸ“Š TensorBoard ç›‘æ§")

# os.makedirs("runs", exist_ok=True)
# TB_PORT = launch_tensorboard(logdir="runs")

# SERVER_IP = "10.66.51.30"

# TENSORBOARD_URL = f"http://{SERVER_IP}:{TB_PORT}"

# st.info(f"TensorBoard æœåŠ¡å·²ç”±åº”ç”¨è‡ªåŠ¨åœ¨åå°å¯åŠ¨ã€‚")

# components.iframe(TENSORBOARD_URL, height=800, scrolling=True)